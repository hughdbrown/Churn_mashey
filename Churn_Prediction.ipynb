{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://mashey.io/wp-content/uploads/2016/02/Mashey-Logo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Business Value of Churn Prediciton and Customer Retention\n",
    "#### Samuel Sherman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Customer acquisition management is the set of methodologies and systems to manage customer prospects and inquiries generated by a variety of marketing techniques.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer Retention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Customer retention is the activity that a selling organization undertakes in order to reduce customer defections. Successful customer retention starts with the first contact an organization has with a customer and continues throughout the entire lifetime of a relationship.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where should a company invest the majority of their efforts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer this question, we need to examine the cost of each and whether that will outweigh benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://sellup.net/wp-content/uploads/2016/03/Screen-Shot-2016-03-15-at-10.40.25-AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of companies invest in customer acquisition, despite having higher costs. The effort to acquire one new customer can cost up to 5 times as much as an effort to retain an existing customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://blog.web-media.co.uk/wp-content/uploads/2015/03/existing-new-customer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For products and services offered, a company expect to gain new sales with a higher probability from existing customers than a brand new customer. However, this is a double edged sword. Your customers need to be happy with their current services in order to consider upselling. Let's examine why a customer might leave the products and services offered by a company.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.r-2w.com/blog/wp-content/uploads/2010/10/Picture-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly some value in efforts to improve customer satisfaction and retention. Now that I have made my case for customer retention, I am going to apply a use case scenario in the field of data science for churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, average_precision_score, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedShuffleSplit, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from unbalanced_dataset.over_sampling import SMOTE\n",
    "from unbalanced_dataset.under_sampling import UnderSampler\n",
    "from scipy import interp\n",
    "from scipy.io.arff import loadarff\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def prepare_data(filename):\n",
    "    churn = loadarff(filename)\n",
    "    churn_df = pd.DataFrame(churn[0])\n",
    "    \n",
    "    # Clean up categorical columns\n",
    "    churn_df['LEAVE'] = (churn_df['LEAVE'] == 'LEAVE').astype(int)\n",
    "    churn_df['COLLEGE'] = (churn_df['COLLEGE'] == \"one\").astype(int)\n",
    "    churn_df = pd.concat([churn_df,pd.get_dummies(churn_df.REPORTED_SATISFACTION)], axis = 1)\n",
    "    churn_df.drop('avg', axis = 1, inplace = True)\n",
    "    churn_df = pd.concat([churn_df,pd.get_dummies(churn_df.REPORTED_USAGE_LEVEL)], axis = 1)\n",
    "    churn_df.drop('avg', axis = 1, inplace = True)\n",
    "    churn_df = pd.concat([churn_df,pd.get_dummies(churn_df.CONSIDERING_CHANGE_OF_PLAN)], axis = 1)\n",
    "    churn_df.drop('never_thought', axis = 1, inplace = True)\n",
    "    churn_df.drop('REPORTED_SATISFACTION', axis = 1, inplace = True)\n",
    "    churn_df.drop('REPORTED_USAGE_LEVEL', axis = 1, inplace = True)\n",
    "    churn_df.drop('CONSIDERING_CHANGE_OF_PLAN', axis = 1, inplace = True)\n",
    "    \n",
    "    # set label array\n",
    "    y = churn_df.pop('LEAVE').values\n",
    "    \n",
    "    # set feature matrix\n",
    "    X = churn_df.values\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, churn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am defining a function to prepare the dataset to be modeled. Basically, I am taking any predictive or independent variables and representing them as a 2d array or matrix. For categorical variables with more than one class, each class will need to be pivoted out and represented as TRUE or FALSE (1 or 0). Categorical variables with only two classes need to be represented as either 1 or 0 or TRUE or FALSE. Continuous variables can be left as is. Additionally, I extract out the dependent or response variable as a 1d array with values TRUE or FALSE. To be clear, TRUE and FALSE are specific class types in python that are equivalent to 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, average_precision_score, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedShuffleSplit, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from unbalanced_dataset.over_sampling import SMOTE\n",
    "from unbalanced_dataset.under_sampling import UnderSampler\n",
    "from scipy import interp\n",
    "from scipy.io.arff import loadarff\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def prepare_data(filename):\n",
    "    churn_df = pd.read_csv(filename)\n",
    "    \n",
    "    # Clean up categorical columns\n",
    "    churn_df['Churn?'] = (churn_df['Churn?'] == 'True.').astype(int)\n",
    "    yes_no_cols = [\"Int'l Plan\", \"VMail Plan\"]\n",
    "    churn_df[yes_no_cols] = (churn_df[yes_no_cols] == \"yes\").astype(int)\n",
    "    \n",
    "    # set label array\n",
    "    y = churn_df.pop('Churn?').values\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    churn_df = churn_df.drop(['State', 'Area Code', 'Phone'], axis=1)\n",
    "    \n",
    "    # set feature matrix\n",
    "    X = churn_df.values\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_logistic_churn(X, y):\n",
    "    AUC, AUC2, thresholds, recall, precision = [], [], [], [], []\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    #Statified K fold\n",
    "    skf = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "    for train_index, test_index in skf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test= y[train_index], y[test_index]\n",
    "        \n",
    "        #Oversampling of unbalanced dataset\n",
    "        #sm = SMOTE(kind = 'regular', verbose = True)\n",
    "        #X_train, y_train = sm.fit_transform(X_train, y_train)\n",
    "        #X_train, y_train = sm.fit_transform(X_train, y_train)\n",
    "        \n",
    "        #Undersampling of unbalanced dataset\n",
    "        #u = UnderSampler()\n",
    "        #X_train, y_train = u.fit_transform(X_train, y_train)\n",
    "        \n",
    "        # Initialize a classifier \n",
    "        clf = LR(random_state = 2, n_jobs = -1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict_proba(X_test)\n",
    "        pred2 = clf.predict(X_test)\n",
    "        \n",
    "        #Evaluate\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, pred[:,1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        AUC.append(roc_auc_score(y_test, pred[:,1]))\n",
    "        AUC2.append(average_precision_score(y_test, pred[:,1]))\n",
    "        recall.append(recall_score(y_test, pred2))\n",
    "        precision.append(precision_score(y_test, pred2))\n",
    "\n",
    "    mean_tpr /= len(skf)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    return recall, AUC, precision, AUC2, mean_fpr, mean_tpr, thresholds, pred2, y_test, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I define a function to apply a logistic regression model with stratified k-fold cross validation. I derive specific metrics for recall, precision, area under the ROC curve, and area under the precision-recall curve. A stratified cross validation will keep the same proportion of classes within the dataset. This is important because you want you model to perform well on real data. Therefore, your out of sample data will be representative of the real data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~scsherm/106.embed\" height=\"680px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.tools import FigureFactory as FF \n",
    "py.sign_in('scsherm', 'ml0wer7f1s')\n",
    "\n",
    "X, y, churn_df = prepare_data('data/churn.arff')\n",
    "(log_recall, log_AUC, log_precision, log_AUC2, log_mean_fpr, log_mean_tpr, \n",
    " log_thresholds, log_pred2, log_y_test, log_clf) = run_logistic_churn(X, y)\n",
    "\n",
    "df = pd.DataFrame(churn_df.columns.values)\n",
    "df['Beta_coefficients'] = np.exp(log_clf.coef_[0])\n",
    "df.rename(columns = {0:'Feature'}, inplace = True)\n",
    "\n",
    "table_churn = FF.create_table(df)\n",
    "py.iplot(table_churn, filename='coef_table_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, applying the model I examine the beta coeficients to determine what features are most predictive. I can conclude that overage use on particular phone is most predictive of a customer churning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~scsherm/112.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.linspace(0,1)\n",
    "\n",
    "data = [go.Scatter(x = log_mean_fpr, y = log_mean_tpr, \n",
    "                   mode = 'lines', \n",
    "                   name = 'ROC'),\n",
    "        go.Scatter(x = v, y = v, \n",
    "                   mode = 'lines', \n",
    "                   name = '50/50 mark')]\n",
    "\n",
    "layout = go.Layout(title='Logistic Regression ROC - AUC = {}'.format(np.mean(log_AUC)),\n",
    "    xaxis=dict(title='False Postive Rate',titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f')),\n",
    "    yaxis=dict(title='True Postive Rate',titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f')))\n",
    "LOG_ROC_Churn = go.Figure(data=data, layout=layout)    \n",
    "py.iplot(LOG_ROC_Churn, filename='LOG_ROC_Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.61530702804884019,\n",
       " 0.69474837249117405,\n",
       " 0.64051475854377904,\n",
       " 0.67995974683914839)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(log_recall), np.mean(log_AUC), np.mean(log_precision), np.mean(log_AUC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now examine the receiver operator characteristic curve and the area under the curve to measure the performance of my model on out of sample data. Other metrics such as recall, precision, and area under the precision-recall curve can help provide information about how the model is performing on positive classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_rf_churn(X, y):\n",
    "    AUC, AUC2, thresholds, recall, precision = [], [], [], [], []\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    #Statified K fold\n",
    "    skf = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "    for train_index, test_index in skf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test= y[train_index], y[test_index]\n",
    "        \n",
    "        #Oversampling of unbalanced dataset\n",
    "        #sm = SMOTE(kind = 'regular', verbose = True)\n",
    "        #X_train, y_train = sm.fit_transform(X_train, y_train)\n",
    "        #X_train, y_train = sm.fit_transform(X_train, y_train)\n",
    "        \n",
    "        #Undersampling of unbalanced dataset\n",
    "        #u = UnderSampler()\n",
    "        #X_train, y_train = u.fit_transform(X_train, y_train)\n",
    "        \n",
    "        # Initialize a classifier \n",
    "        clf = RF(random_state = 2, n_estimators = 100, n_jobs = -1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict_proba(X_test)\n",
    "        pred2 = clf.predict(X_test)\n",
    "        \n",
    "        #Evaluate\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, pred[:,1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        AUC.append(roc_auc_score(y_test, pred[:,1]))\n",
    "        AUC2.append(average_precision_score(y_test, pred[:,1]))\n",
    "        recall.append(recall_score(y_test, pred2))\n",
    "        precision.append(precision_score(y_test, pred2))\n",
    "\n",
    "    mean_tpr /= len(skf)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    return recall, AUC, precision, AUC2, mean_fpr, mean_tpr, thresholds, pred2, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My random forest function will run a random forest model with stratified k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~scsherm/108.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rf_recall, rf_AUC, rf_precision, rf_AUC2, rf_mean_fpr, \n",
    " rf_mean_tpr, rf_thresholds, rf_pred2, rf_y_test) = run_rf_churn(X, y)\n",
    "\n",
    "v = np.linspace(0,1)\n",
    "\n",
    "data = [go.Scatter(x = rf_mean_fpr, y = rf_mean_tpr, \n",
    "                   mode = 'lines', \n",
    "                   name = 'ROC'),\n",
    "        go.Scatter(x = v, y = v, \n",
    "                   mode = 'lines', \n",
    "                   name = '50/50 mark')]\n",
    "\n",
    "layout = go.Layout(title='Random Forest ROC - AUC = {}'.format(np.mean(rf_AUC)),\n",
    "    xaxis=dict(title='False Postive Rate',titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f')),\n",
    "    yaxis=dict(title='True Postive Rate',titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f')))\n",
    "RF_ROC_Churn = go.Figure(data=data, layout=layout)    \n",
    "py.iplot(RF_ROC_Churn, filename='RF_ROC_Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.70970400760262387,\n",
       " 0.75817368190627099,\n",
       " 0.67898533373443859,\n",
       " 0.71813651909465792)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rf_recall), np.mean(rf_AUC), np.mean(rf_precision), np.mean(rf_AUC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I examine the same metrics as before to measure the performance of the model. I can see that area under the curve has increased and therefore the model is performing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "def run_gradient_boosted_gsearch(X, y):\n",
    "    AUC, AUC2, thresholds, recall, precision = [], [], [], [], []\n",
    "    best_params = []\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    #Statified K fold\n",
    "    skf = StratifiedKFold(y, n_folds = 5, shuffle = True)\n",
    "    for train_index, test_index in skf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test= y[train_index], y[test_index]\n",
    "        \n",
    "        #Oversampling of unbalanced dataset\n",
    "        #sm = SMOTE(kind = 'regular', verbose = True)\n",
    "        #X_train, y_train = sm.fit_transform(X_train, y_train)\n",
    "        #X_train, y_train = sm.fit_transform(X_train, y_train)\n",
    "        \n",
    "        #Undersampling of unbalanced dataset\n",
    "        #u = UnderSampler()\n",
    "        #X_train, y_train = u.fit_transform(X_train, y_train)\n",
    "        \n",
    "        #setup paramgrid for grid search\n",
    "        param_grid = [{'learning_rate': [.01], 'n_estimators': [500, 1000, 2000], 'max_depth': [5]}]\n",
    "        #clf = xgb.XGBClassifier(learning_rate = 0.01, max_depth = 5, n_estimators = 200)\n",
    "        xgb_model = xgb.XGBClassifier()\n",
    "        clf = GridSearchCV(xgb_model, param_grid, verbose = 2, cv = 2, n_jobs = -1, scoring = 'roc_auc')\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict_proba(X_test) \n",
    "        pred2 = clf.predict(X_test)\n",
    "        best_params.append(clf.best_params_)\n",
    "        \n",
    "        #Evaluate\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, pred[:,1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        AUC.append(roc_auc_score(y_test, pred[:,1]))\n",
    "        AUC2.append(average_precision_score(y_test, pred[:,1]))\n",
    "        recall.append(recall_score(y_test, pred2))\n",
    "        precision.append(precision_score(y_test, pred2))\n",
    "\n",
    "    mean_tpr /= len(skf)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    return recall, AUC, precision, AUC2, mean_fpr, mean_tpr, thresholds, pred2, y_test, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final model will be a gradient boosted model run through a function. I am implementing stratified k-fold cross validation and a grid search of the hyperparameters. Gradient boosted models are prone to overfit their data. So running different combinations of hyperparamters to find the best performance on out of sample data is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   4.8s[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   5.0s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -   8.3s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -   8.4s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  14.7s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  14.6s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   14.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   5.7s[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   5.9s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -  10.0s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -  10.0s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  16.7s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  16.7s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   16.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   5.8s[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   5.8s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -   9.9s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -   9.9s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  16.9s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  16.7s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   17.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   5.5s[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   5.5s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -   9.4s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -   9.3s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  16.3s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  16.1s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=500, learning_rate=0.01, max_depth=5 ...............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=1000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] n_estimators=2000, learning_rate=0.01, max_depth=5 ..............\n",
      "[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   5.9s[CV] ...... n_estimators=500, learning_rate=0.01, max_depth=5 -   6.0s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -  10.7s[CV] ..... n_estimators=1000, learning_rate=0.01, max_depth=5 -  10.7s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  18.2s[CV] ..... n_estimators=2000, learning_rate=0.01, max_depth=5 -  17.9s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   18.3s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~scsherm/110.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gb_recall, gb_AUC, gb_precision, gb_AUC2, gb_mean_fpr, \n",
    " gb_mean_tpr, gb_thresholds, gb_pred2, gb_y_test, best_params) = run_gradient_boosted_gsearch(X, y)\n",
    "\n",
    "v = np.linspace(0,1)\n",
    "\n",
    "data = [go.Scatter(x = gb_mean_fpr, y = gb_mean_tpr, \n",
    "                   mode = 'lines', \n",
    "                   name = 'ROC'),\n",
    "        go.Scatter(x = v, y = v, \n",
    "                   mode = 'lines', \n",
    "                   name = '50/50 mark')]\n",
    "\n",
    "    \n",
    "layout = go.Layout(title='Gradient Boosted ROC - AUC = {}'.format(np.mean(gb_AUC)),\n",
    "    xaxis=dict(title='False Postive Rate',titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f')),\n",
    "    yaxis=dict(title='True Postive Rate',titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f')))\n",
    "GB_ROC_Churn = go.Figure(data=data, layout=layout)    \n",
    "py.iplot(GB_ROC_Churn, filename='GB_ROC_Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74116480850504918,\n",
       " 0.77129704429004387,\n",
       " 0.6788894476463907,\n",
       " 0.73043179113843737)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gb_recall), np.mean(gb_AUC), np.mean(gb_precision), np.mean(gb_AUC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500},\n",
       " {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500},\n",
       " {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500},\n",
       " {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500},\n",
       " {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model appears to be performing the best. I can see that all the hyperparamters for each fold have converged on the same value and can have confidence that I have reached my maximum performance for a gradient bossted model. The area under the curve is at 77% and is higher than both previous algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~scsherm/130.embed\" height=\"600px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "log_cfmat = confusion_matrix(log_y_test,log_pred2)\n",
    "rf_cfmat = confusion_matrix(rf_y_test,rf_pred2)\n",
    "gb_cfmat = confusion_matrix(gb_y_test,gb_pred2)\n",
    "\n",
    "trace1 = go.Heatmap(x=['0', '1'],y=['0', '1'],z=log_cfmat,autocolorscale=False,\n",
    "    colorscale=[[0, 'rgb(220,220,220)'], [0.2, 'rgb(245,195,157)'], [0.4, 'rgb(245,160,105)'], [1, 'rgb(178,10,28)']],\n",
    "    name='Trace 0, y; Trace 1, y; Trace 2, y',uid='eef847',xaxis='x',xsrc='scsherm:115:a351a4',yaxis='y',\n",
    "    ysrc='scsherm:115:da265a',zauto=False,zmax=1500,zmin=400,zsrc='scsherm:115:c72e21,6fdddc')\n",
    "trace2 = go.Heatmap(x=['0', '1'],y=['0', '1'],z=rf_cfmat,autocolorscale=False,\n",
    "    colorscale=[[0, 'rgb(220,220,220)'], [0.2, 'rgb(245,195,157)'], [0.4, 'rgb(245,160,105)'], [1, 'rgb(178,10,28)']],\n",
    "    name='Trace 0, y; Trace 1, y; Trace 2, y',uid='08b7a9',xaxis='x2',xsrc='scsherm:115:a351a4',yaxis='y2',\n",
    "    ysrc='scsherm:115:da265a',zmax=1500,zmin=400,zsrc='scsherm:115:e30fc6,d7f25c')\n",
    "trace3 = go.Heatmap(x=['0', '1'],y=['0', '1'],z=gb_cfmat,autocolorscale=False,\n",
    "    colorscale=[[0, 'rgb(220,220,220)'], [0.2, 'rgb(245,195,157)'], [0.4, 'rgb(245,160,105)'], [1, 'rgb(178,10,28)']],\n",
    "    name='Trace 0, y; Trace 1, y; Trace 2, y',uid='acf982',xaxis='x3',xsrc='scsherm:115:a351a4',yaxis='y3',\n",
    "    ysrc='scsherm:115:da265a',zauto=False,zmax=1500,zmin=400,zsrc='scsherm:115:996093,c0fe9a')\n",
    "data = go.Data([trace1, trace2, trace3])\n",
    "\n",
    "layout = go.Layout(annotations=go.Annotations([go.Annotation(x=0,y=1,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(log_cfmat[1][0])),\n",
    "        go.Annotation(x=0,y=0,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(log_cfmat[0][0])),\n",
    "        go.Annotation(x=1,y=0,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(log_cfmat[0][1])),\n",
    "        go.Annotation(x=1,y=1,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(log_cfmat[1][1])),\n",
    "        go.Annotation(x=0,y=0,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(rf_cfmat[0][0]),xref='x2',yref='y2'),\n",
    "        go.Annotation(x=0,y=1,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(rf_cfmat[1][0]),xref='x2',yref='y2'),\n",
    "        go.Annotation(x=1,y=0,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(rf_cfmat[0][1]),xref='x2',yref='y2'),\n",
    "        go.Annotation(x=1,y=1,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(rf_cfmat[1][1]),xref='x2',yref='y2'),\n",
    "        go.Annotation(x=0,y=0,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(gb_cfmat[0][0]),xref='x3',yref='y3'),\n",
    "        go.Annotation(x=0,y=1,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(gb_cfmat[1][0]),xref='x3',yref='y3'),\n",
    "        go.Annotation(x=1,y=0,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(gb_cfmat[0][1]),xref='x3',yref='y3'),\n",
    "        go.Annotation(x=1,y=1,font=go.Font(color='rgb(255, 255, 255)',\n",
    "                family='Roboto, sans-serif',size=14),showarrow=False,text=str(gb_cfmat[1][1]),xref='x3',yref='y3')]),\n",
    "    height=600,\n",
    "    title='Confusion Matrices',\n",
    "    width=1000,\n",
    "    xaxis=go.XAxis(title = 'Logistic - Predicted', anchor='y',autorange=True,\n",
    "                   domain=[0, 0.2888888888888889],range=[-0.5, 1.5],type='category'),\n",
    "    xaxis2=go.XAxis(title = 'Random Forest - Predicted', anchor='y2',autorange=True,\n",
    "                    domain=[0.35555555555555557, 0.6444444444444445],range=[-0.5, 1.5], type='category'),\n",
    "    xaxis3=go.XAxis(title = 'Gradient Boosted - Predicted', anchor='y3',autorange=True,domain=[0.7111111111111111, 1],\n",
    "                    range=[-0.5, 1.5],type='category'),\n",
    "    yaxis=go.YAxis(anchor='x',autorange=True,domain=[0, 1],range=[-0.5, 1.5],title='Actual Class',type='category'),\n",
    "    yaxis2=go.YAxis(anchor='x2',autorange=True,domain=[0, 1],range=[-0.5, 1.5],type='category'),\n",
    "    yaxis3=go.YAxis(anchor='x3',autorange=True,domain=[0, 1],range=[-0.5, 1.5],type='category'))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I examine the confusion matrices for all models to see how the models are distributing the predicted classes, as compared to their real values. Gradient boosted, again, is performing best. The total accuracy is higher than the other models. The total count of false labels is less than the other models. The logistic regression model has a large spread through the falsely classified labels. The random forest model is performing similarly, but the gradient boosted model has a higher count for true labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "html {\n",
       "  font-size: 62.5% !important; }\n",
       "body {\n",
       "  font-size: 1.5em !important; /* currently ems cause chrome bug misinterpreting rems on body element */\n",
       "  line-height: 1.6 !important;\n",
       "  font-weight: 400 !important;\n",
       "  font-family: \"Raleway\", \"HelveticaNeue\", \"Helvetica Neue\", Helvetica, Arial, sans-serif !important;\n",
       "  color: #222 !important; }\n",
       "\n",
       "div{ border-radius: 0px !important;  }\n",
       "div.CodeMirror-sizer{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_area{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "div.out_prompt_overlay:hover{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_prompt:hover{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  color: #333 !important;\n",
       "  margin-top: 0 !important;\n",
       "  margin-bottom: 2rem !important;\n",
       "  font-weight: 300 !important; }\n",
       "h1 { font-size: 4.0rem !important; line-height: 1.2 !important;  letter-spacing: -.1rem !important;}\n",
       "h2 { font-size: 3.6rem !important; line-height: 1.25 !important; letter-spacing: -.1rem !important; }\n",
       "h3 { font-size: 3.0rem !important; line-height: 1.3 !important;  letter-spacing: -.1rem !important; }\n",
       "h4 { font-size: 2.4rem !important; line-height: 1.35 !important; letter-spacing: -.08rem !important; }\n",
       "h5 { font-size: 1.8rem !important; line-height: 1.5 !important;  letter-spacing: -.05rem !important; }\n",
       "h6 { font-size: 1.5rem !important; line-height: 1.6 !important;  letter-spacing: 0 !important; }\n",
       "\n",
       "@media (min-width: 550px) {\n",
       "  h1 { font-size: 5.0rem !important; }\n",
       "  h2 { font-size: 4.2rem !important; }\n",
       "  h3 { font-size: 3.6rem !important; }\n",
       "  h4 { font-size: 3.0rem !important; }\n",
       "  h5 { font-size: 2.4rem !important; }\n",
       "  h6 { font-size: 1.5rem !important; }\n",
       "}\n",
       "\n",
       "p {\n",
       "  margin-top: 0 !important; }\n",
       "  \n",
       "a {\n",
       "  color: #1EAEDB !important; }\n",
       "a:hover {\n",
       "  color: #0FA0CE !important; }\n",
       "  \n",
       "code {\n",
       "  padding: .2rem .5rem !important;\n",
       "  margin: 0 .2rem !important;\n",
       "  font-size: 90% !important;\n",
       "  white-space: nowrap !important;\n",
       "  background: #F1F1F1 !important;\n",
       "  border: 1px solid #E1E1E1 !important;\n",
       "  border-radius: 4px !important; }\n",
       "pre > code {\n",
       "  display: block !important;\n",
       "  padding: 1rem 1.5rem !important;\n",
       "  white-space: pre !important; }\n",
       "  \n",
       "button{ border-radius: 0px !important; }\n",
       ".navbar-inner{ background-image: none !important;  }\n",
       "select, textarea{ border-radius: 0px !important; }\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "import urllib2\n",
    "HTML(urllib2.urlopen('http://bit.ly/1Bf5Hft').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
